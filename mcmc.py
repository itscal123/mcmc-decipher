"""
File containing functions that run MCMC to find decryption cipher. 
"""
from matplotlib.pyplot import step
import utils
import scipy.stats as st
import autograd.numpy as np
from autograd import grad


def mcmc(decrypt_key, encoded_text, rng, iters=50000):
    """
    Function that uses the Metroplis-Hastings algorithm for our Markov Chain 
    Monte Carlo implementation.
    ----------
    params:
        decrypt_key (str): String representation of the decryption cipher
        encoded_text (str): Passed encrypted text
        rng (generator): Seeded random number generator
        iters (int): Number of MCMC iterations, default 50,000
    returns:
        decrypt_key (str): Decryption cipher sampled from MCMC
    """
    # Get the character frequencies
    freqDict = utils.loadFreqDict()

    print('Running Markov Chain Monte Carlo')
    # Run the MCMC for the passed number of iterations
    for i in range(iters):
        # Make proposal
        proposal = utils.proposeKey(decrypt_key, rng)
        # Calculate the quality of the current decryption cipher
        curr_score = utils.score(decrypt_key, encoded_text, freqDict)
        # Calculate the quality of the proposed decryption cipher
        new_score = utils.score(proposal, encoded_text, freqDict)

        # Calculate the acceptance probability (ratio between the current
        # score and proposed score)
        accept_ratio = min(1, np.exp(new_score - curr_score))

        # Sample randomly from [0,1]
        randnum = rng.uniform()

        # Boolean flag denoting if we should accept the proposal
        acceptProposal = False if randnum > accept_ratio else True

        # Use new decryption cipher if we accept proposal
        if acceptProposal:
            decrypt_key = proposal

        # Print every 500th iteration
        if (i+1) == 1 or (i+1) % 500 == 0:
            decode = utils.applyKey(decrypt_key, encoded_text)
            score = utils.score(decrypt_key, encoded_text, freqDict)
            print(f'Iteration {i+1:>5} -> Score: {score:.2f}\n')
            print(f'Decrypted text:\n{decode}\n')
    
    # Return the final decryption cipher
    return decrypt_key

# Initial position should be initial decryption cipher guess?
def hamiltonian_monte_carlo(
    n_samples,
    potential,
    position,
    path_len,
    step_size,
    encoded_text
    ):
    """
    Function that uses Hamiltonian Monte Carlo to sample decryption ciphers.
    -------------
    params:
    n_samples (int)
        Number of samples (i.e., iterations) to run
    
    potential (callable)
        Potential energy representation of the decryption cipher (i.e., negative log likelihood?)

    position (np.array)
        Position to sample from

    path_len (float)
        Length of each integration path. Smaller is faster and more correlated

    step_size (float)
        Size of each integration step. Smaller is slower and more accurate
    -------------
    returns:
    position (np.array)
        Final position (i.e., decryption cipher) generated by Hamiltonian Monte Carlo
    """
    # Get the character frequencies
    freqDict = utils.loadFreqDict()

    # Initialize autograd
    dVdq = grad(potential)

    # Initialize momementum sampler
    momentum = st.norm(0,1)

    # Initial position is 26d vector? Fill in comments later
    size = (n_samples, ) + position.shape[:1]

    for p0 in momentum.rvs(size=size):
        # Integrate over path to generate new position and momentum
        q_new, p_new = leapfrog(
            position,
            p0,
            dVdq,
            path_len,
            step_size
        )

        # Calculate log likelihood of current position and proposed position
        curr_log_p = 0  # TODO
        new_log_p = 0   # TODO

        # Apply Metroplis Hastings acceptance criterion
        if np.log(rng.rand()) < curr_log_p - new_log_p:
            position = q_new


    # Return the final position (decryption cipher)
    return position
        

def leapfrog(q, p, dVdq, path_len, step_size):
    """
    Helper function for Hamiltonian Monte Carlo that uses leapfrog integration to numerically integrate
    differential equations. To avoid the accumulation of systematic errors, update the momentum using
    a half step, then the position at a full step. Finish updating p with another half step. 
    The loop you can perform steps normally (investigate further). Finally, flip the momentum so that our process
    has a non-zero, symmetric, probability of moving back to some initial distribution.
    ----------
    params:
        q (np.float64): initial position
        p (np.float64): initial momentum
        dVdq (callable): gradient of the velocity
        path_len (float): how long to integrate for
        step_size (float): how long each integration step should be

    returns:
        q, p (np.float64, np.float64): updated position and momentum
    """
    # Create shallow copies of the values q and p
    q = np.copy(q)
    p = np.copy(p)

    # Calculate velocity
    velocity = dVdq(q)

    # Take step in direction of the gradient of the velocity * step_size
    for _ in range(int(path_len / step_size) - 1):
        p -= step_size * velocity / 2   # half step
        q += step_size * p              # whole step
        velocity = dVdq(q)              # recalculate velocity
        p -= step_size * velocity / 2   # half step

    # Momentum flip
    return q, -p